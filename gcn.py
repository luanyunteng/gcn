2820     loss is:  1.1548982
**************
accuracy:  0.7785
**************
2830     loss is:  1.1420449
**************
accuracy:  0.78450006
**************
2840     loss is:  1.1420271
**************
accuracy:  0.78425
**************
2850     loss is:  1.1348665
**************
accuracy:  0.7835001
**************
2860     loss is:  1.1401318
**************
accuracy:  0.7822501
**************
2870     loss is:  1.1366321
**************
accuracy:  0.78700006
**************
2880     loss is:  1.1285498
**************
accuracy:  0.7850001
**************
2890     loss is:  1.1308801
**************
accuracy:  0.78700006
**************
2900     loss is:  1.127687
**************
accuracy:  0.78575003
**************
2910     loss is:  1.1242542
**************
accuracy:  0.78550005
**************
2920     loss is:  1.1188291
**************
accuracy:  0.78650004
**************
2930     loss is:  1.109581
**************
accuracy:  0.7867501
**************
2940     loss is:  1.1168106
**************
accuracy:  0.7867501
**************
2950     loss is:  1.1199448
**************
accuracy:  0.78975004
**************
2960     loss is:  1.1104302
**************
accuracy:  0.788
**************
2970     loss is:  1.1153922
**************
accuracy:  0.7900001
**************
2980     loss is:  1.115265
**************
accuracy:  0.7885
**************
2990     loss is:  1.1086357
**************
accuracy:  0.7900001
**************
3000     loss is:  1.1038847
**************
accuracy:  0.7860001
**************
3010     loss is:  1.1063886
**************
accuracy:  0.7897501
**************
3020     loss is:  1.1036336
**************
accuracy:  0.7865001
**************
3030     loss is:  1.0951165
**************
accuracy:  0.7925001
**************
3040     loss is:  1.0988641
**************
accuracy:  0.7872501
**************
3050     loss is:  1.1014302
**************
accuracy:  0.7912501
**************
3060     loss is:  1.0789834
**************
accuracy:  0.7922501
**************
3070     loss is:  1.0987566
**************
accuracy:  0.78975016
**************
3080     loss is:  1.0930836
**************
accuracy:  0.7902501
**************
3090     loss is:  1.0882325
**************
accuracy:  0.7915001
**************
3100     loss is:  1.0867339
**************
accuracy:  0.79325
**************
3110     loss is:  1.0854445
**************
accuracy:  0.79350007
**************
3120     loss is:  1.0848234
**************
accuracy:  0.7932501
**************
3130     loss is:  1.0876863
**************
accuracy:  0.79050004
**************
3140     loss is:  1.0849652
**************
accuracy:  0.7925001
**************
3150     loss is:  1.067548
**************
accuracy:  0.78925
**************
3160     loss is:  1.0743546
**************
accuracy:  0.79225004
**************
3170     loss is:  1.0729519
**************
accuracy:  0.7932501
**************
3180     loss is:  1.0781606
**************
accuracy:  0.79150003
**************
3190     loss is:  1.0688181
**************
accuracy:  0.79350007
**************
3200     loss is:  1.0787638
**************
accuracy:  0.79350007
**************
3210     loss is:  1.0729536
**************
accuracy:  0.78900003
**************
3220     loss is:  1.0717999
**************
accuracy:  0.7945001
**************
3230     loss is:  1.0691838
**************
accuracy:  0.7945001
**************
3240     loss is:  1.0580451
**************
accuracy:  0.79475003
**************
3250     loss is:  1.0628242
**************
accuracy:  0.79375005
**************
3260     loss is:  1.0617037
**************
accuracy:  0.7942501
**************
3270     loss is:  1.0708328
**************
accuracy:  0.7965001
**************
3280     loss is:  1.0655587
**************
accuracy:  0.7835001
**************
3290     loss is:  1.0574584
**************
accuracy:  0.7942501
**************
3300     loss is:  1.0592427
**************
accuracy:  0.7935001
**************
3310     loss is:  1.061898
**************
accuracy:  0.79175013
**************
3320     loss is:  1.0571363
**************
accuracy:  0.79125005
**************
3330     loss is:  1.0541577
**************
accuracy:  0.78675
**************
3340     loss is:  1.0652716
**************
accuracy:  0.78300005
**************
3350     loss is:  1.0632885
**************
accuracy:  0.79450005
**************
3360     loss is:  1.0527191
**************
accuracy:  0.79600006
**************
3370     loss is:  1.0670111
**************
accuracy:  0.79200006
**************
3380     loss is:  1.0519342
**************
accuracy:  0.79575
**************
3390     loss is:  1.046779
**************
accuracy:  0.79625005
**************
3400     loss is:  1.045481
**************
accuracy:  0.7950001
**************
3410     loss is:  1.0583619
**************
accuracy:  0.7950001
**************
3420     loss is:  1.0544891
**************
accuracy:  0.7945001
**************
3430     loss is:  1.054918
**************
accuracy:  0.7945001
**************
3440     loss is:  1.0382018
**************
accuracy:  0.79300004
**************
3450     loss is:  1.0412647
**************
accuracy:  0.79650015
**************
3460     loss is:  1.040832
**************
accuracy:  0.78900003
**************
3470     loss is:  1.03408
**************
accuracy:  0.7932501
**************
3480     loss is:  1.2757785
**************
accuracy:  0.7512501
**************
3490     loss is:  1.0873258
**************
accuracy:  0.79100007
**************
3500     loss is:  1.040193
**************
accuracy:  0.7832501
**************
3510     loss is:  1.042256
**************
accuracy:  0.7927501
**************
3520     loss is:  1.0299647
**************
accuracy:  0.7950001
**************
3530     loss is:  1.0301533
**************
accuracy:  0.7925001
**************
3540     loss is:  1.0312154
**************
accuracy:  0.7945001
**************
3550     loss is:  1.044174
**************
accuracy:  0.79175013
**************
3560     loss is:  1.0267757
**************
accuracy:  0.79525006
**************
3570     loss is:  1.029098
**************
accuracy:  0.79400015
**************
3580     loss is:  1.0344654
**************
accuracy:  0.79675007
**************
3590     loss is:  1.0249286
**************
accuracy:  0.79750013
**************
3600     loss is:  1.024813
**************
accuracy:  0.79850006
**************
3610     loss is:  1.0208397
**************
accuracy:  0.7970001
**************
3620     loss is:  1.0265057
**************
accuracy:  0.79275006
**************
3630     loss is:  1.0228665
**************
accuracy:  0.79375005
**************
3640     loss is:  1.0533329
**************
accuracy:  0.79350007
**************
3650     loss is:  1.0450964
**************
accuracy:  0.79175013
**************
3660     loss is:  1.0372198
**************
accuracy:  0.77825004
**************
3670     loss is:  1.0633391
**************
accuracy:  0.7915001
**************
3680     loss is:  1.0402927
**************
accuracy:  0.79475
**************
3690     loss is:  1.0248758
**************
accuracy:  0.78925014
**************
3700     loss is:  1.0215908
**************
accuracy:  0.78900003
**************
3710     loss is:  1.0271854
**************
accuracy:  0.7967501
**************
3720     loss is:  1.0152869
**************
accuracy:  0.79525006
**************
3730     loss is:  1.0145015
**************
accuracy:  0.79850006
**************
3740     loss is:  1.0169482
**************
accuracy:  0.7972501
**************
3750     loss is:  1.0442612
**************
accuracy:  0.78550005
**************
3760     loss is:  1.0084032
**************
accuracy:  0.79025006
**************
3770     loss is:  1.0089853
**************
accuracy:  0.79450005
**************
3780     loss is:  1.0073599
**************
accuracy:  0.79325
**************
3790     loss is:  1.0011964
**************
accuracy:  0.79649997
**************
3800     loss is:  1.125585
**************
accuracy:  0.67550015
**************
3810     loss is:  3.255509
**************
accuracy:  0.50125
**************
3820     loss is:  1.5938153
**************
accuracy:  0.67525
**************
3830     loss is:  1.252809
**************
accuracy:  0.74725
**************
3840     loss is:  1.1913369
**************
accuracy:  0.7207501
**************
3850     loss is:  1.0749127
**************
accuracy:  0.78075004
**************
3860     loss is:  1.0589578
**************
accuracy:  0.7895001
**************
3870     loss is:  1.0352279
**************
accuracy:  0.7955001
**************
3880     loss is:  1.0244744
**************
accuracy:  0.79400015
**************
3890     loss is:  1.023066
**************
accuracy:  0.79500014
**************
3900     loss is:  1.026853
**************
accuracy:  0.79725003
**************
3910     loss is:  1.0160537
**************
accuracy:  0.7972501
**************
3920     loss is:  1.0144546
**************
accuracy:  0.79675007
**************
3930     loss is:  1.003717
**************
accuracy:  0.79600006
**************
3940     loss is:  1.0087535
**************
accuracy:  0.79575014
**************
3950     loss is:  1.0111104
**************
accuracy:  0.7972501
**************
3960     loss is:  1.0192944
**************
accuracy:  0.79525006
**************
3970     loss is:  1.0081112
**************
accuracy:  0.79725015
**************
3980     loss is:  1.0007734
**************
accuracy:  0.7977501
**************
3990     loss is:  1.0133034
**************
accuracy:  0.7980001
**************
4000     loss is:  1.008037
**************
accuracy:  0.7980001
**************
4010     loss is:  1.0036769
**************
accuracy:  0.7987501
**************
4020     loss is:  1.0109433
**************
accuracy:  0.7985002
**************
4030     loss is:  0.99586743
**************
accuracy:  0.79600006
**************
4040     loss is:  1.0007765
**************
accuracy:  0.79775006
**************
4050     loss is:  1.0049868
**************
accuracy:  0.7972501
**************
4060     loss is:  0.99617255
**************
accuracy:  0.7997501
**************
4070     loss is:  0.99736404
**************
accuracy:  0.79925
**************
4080     loss is:  0.9968884
**************
accuracy:  0.79750013
**************
4090     loss is:  0.9991155
**************
accuracy:  0.7977501
**************
4100     loss is:  0.9987918
**************
accuracy:  0.79700005
**************
4110     loss is:  0.9945238
**************
accuracy:  0.7975001
**************
4120     loss is:  0.9970648
**************
accuracy:  0.79775006
**************
4130     loss is:  0.9952357
**************
accuracy:  0.80000013
**************
4140     loss is:  0.98710793
**************
accuracy:  0.7972501
**************
4150     loss is:  0.9863797
**************
accuracy:  0.79925007
**************
4160     loss is:  0.9859042
**************
accuracy:  0.7987501
**************
4170     loss is:  0.9769486
**************
accuracy:  0.80025005
**************
4180     loss is:  0.9869878
**************
accuracy:  0.79825014
**************
4190     loss is:  0.98327005
**************
accuracy:  0.79900014
**************
4200     loss is:  0.98954666
**************
accuracy:  0.80100006
**************
4210     loss is:  0.98661083
**************
accuracy:  0.8000001
**************
4220     loss is:  0.9853524
**************
accuracy:  0.7987501
**************
4230     loss is:  0.98465115
**************
accuracy:  0.7992501
**************
4240     loss is:  0.9821406
**************
accuracy:  0.80025005
**************
4250     loss is:  0.9786226
**************
accuracy:  0.8000001
**************
4260     loss is:  0.97844255
**************
accuracy:  0.80075014
**************
4270     loss is:  0.9805014
**************
accuracy:  0.79975015
**************
4280     loss is:  0.9747639
**************
accuracy:  0.8000001
**************
4290     loss is:  0.97687095
**************
accuracy:  0.7990001
**************
4300     loss is:  0.9814131
**************
accuracy:  0.80100006
**************
4310     loss is:  0.98359376
**************
accuracy:  0.8020001
**************
4320     loss is:  0.9712079
**************
accuracy:  0.80075
**************
4330     loss is:  0.9808012
**************
accuracy:  0.80000013
**************
4340     loss is:  0.9720188
**************
accuracy:  0.8005001
**************
4350     loss is:  0.97420007
**************
accuracy:  0.8002501
**************
4360     loss is:  0.97260594
**************
accuracy:  0.8017501
**************
4370     loss is:  0.9888633
**************
accuracy:  0.79675007
**************
4380     loss is:  0.9851233
**************
accuracy:  0.7945
**************
4390     loss is:  0.9658005
**************
accuracy:  0.79700017
**************
4400     loss is:  0.96928465
**************
accuracy:  0.8025001
**************
4410     loss is:  0.9666696
**************
accuracy:  0.8032501
**************
4420     loss is:  0.96909547
**************
accuracy:  0.80025005
**************
4430     loss is:  0.9619775
**************
accuracy:  0.7997501
**************
4440     loss is:  0.96728194
**************
accuracy:  0.80200005
**************
4450     loss is:  0.9631695
**************
accuracy:  0.799
**************
4460     loss is:  0.9682149
**************
accuracy:  0.8007501
**************
4470     loss is:  0.9614192
**************
accuracy:  0.80300003
**************
4480     loss is:  0.9648529
**************
accuracy:  0.80025005
**************
4490     loss is:  1.0665376
**************
accuracy:  0.7100001
**************
4500     loss is:  3.052961
**************
accuracy:  0.42575002
**************
4510     loss is:  2.3492138
**************
accuracy:  0.57500005
**************
4520     loss is:  1.652837
**************
accuracy:  0.66025007
**************
4530     loss is:  1.1755818
**************
accuracy:  0.744
**************
4540     loss is:  1.0953088
**************
accuracy:  0.77225006
**************
4550     loss is:  1.0190933
**************
accuracy:  0.7917501
**************
4560     loss is:  0.9965077
**************
accuracy:  0.7995001
**************
4570     loss is:  0.9949935
**************
accuracy:  0.80075
**************
4580     loss is:  0.99124104
**************
accuracy:  0.8022501
**************
4590     loss is:  0.9881654
**************
accuracy:  0.8015001
**************
4600     loss is:  0.9882723
**************
accuracy:  0.8027501
**************
4610     loss is:  0.98538357
**************
accuracy:  0.8030001
**************
4620     loss is:  0.9757498
**************
accuracy:  0.8025001
**************
4630     loss is:  0.9794989
**************
accuracy:  0.80225
**************
4640     loss is:  0.99197453
**************
accuracy:  0.8012501
**************
4650     loss is:  0.9793119
**************
accuracy:  0.80250007
**************
4660     loss is:  0.9742769
**************
accuracy:  0.80175006
**************
4670     loss is:  0.97572196
**************
accuracy:  0.80325
**************
4680     loss is:  0.9675443
**************
accuracy:  0.8045001
**************
4690     loss is:  0.972349
**************
accuracy:  0.80200005
**************
4700     loss is:  0.96857935
**************
accuracy:  0.8032501
**************
4710     loss is:  0.9718489
**************
accuracy:  0.8030001
**************
4720     loss is:  0.96633047
**************
accuracy:  0.8037501
**************
4730     loss is:  0.96531904
**************
accuracy:  0.8022501
**************
4740     loss is:  0.96290624
**************
accuracy:  0.80275005
**************
4750     loss is:  0.96047455
**************
accuracy:  0.8030001
**************
4760     loss is:  0.9627825
**************
accuracy:  0.8032501
**************
4770     loss is:  0.96087146
**************
accuracy:  0.80375004
**************
4780     loss is:  0.9618929
**************
accuracy:  0.8030001
**************
4790     loss is:  0.9589425
**************
accuracy:  0.80425
**************
4800     loss is:  0.9594474
**************
accuracy:  0.80450004
**************
4810     loss is:  0.9537528
**************
accuracy:  0.8032501
**************
4820     loss is:  0.9494475
**************
accuracy:  0.80500007
**************
4830     loss is:  0.96274436
**************
accuracy:  0.8040001
**************
4840     loss is:  0.9599143
**************
accuracy:  0.8047501
**************
4850     loss is:  0.95515454
**************
accuracy:  0.8040001
**************
4860     loss is:  0.95155495
**************
accuracy:  0.80425006
**************
4870     loss is:  0.9543495
**************
accuracy:  0.8060001
**************
4880     loss is:  0.9462304
**************
accuracy:  0.80500007
**************
4890     loss is:  0.9473495
**************
accuracy:  0.8035
**************
4900     loss is:  0.94184804
**************
accuracy:  0.8055001
**************
4910     loss is:  0.95554215
**************
accuracy:  0.8055001
**************
4920     loss is:  0.9472267
**************
accuracy:  0.8052501
**************
4930     loss is:  0.9500236
**************
accuracy:  0.80475
**************
4940     loss is:  0.947717
**************
accuracy:  0.8030001
**************
4950     loss is:  0.94310665
**************
accuracy:  0.80600005
**************
4960     loss is:  0.9486259
**************
accuracy:  0.80475
**************
4970     loss is:  0.9393767
**************
accuracy:  0.8047501
**************
4980     loss is:  0.953471
**************
accuracy:  0.80500007
**************
4990     loss is:  0.93963504
**************
accuracy:  0.80575
**************
lyt@up169:~$ ls
Anaconda3-5.1.0-Linux-x86_64.sh.1       getgraph.py                result_by_graph_2
bazel                                   homework                   static_analysis
bazel-0.4.5-dist.zip                    homework-data-for-student  t2t_data
bazel-0.4.5-installer-darwin-x86_64.sh  inception                  t2t_train
bazel-0.5.4-dist.zip                    inception2.zip             tensor_generator.cc
bazel-0.8.0-dist.zip                    inception_test.py          test_find_critical_path_1.py
bazel-0.9.0-installer-darwin-x86_64.sh  inception_train.py         test_find_critical_path_2.py
connect                                 inception_without_summary  test_find_critical_path.py
connect1.1_linux.zip                    inception.zip              test_partition_inception_32299.py
countop.py                              log                        test.py
examples.desktop                        log_inference_withtrain    work
experiment                              mycert.pem
gcn.py                                  readdata.py
lyt@up169:~$ ls
Anaconda3-5.1.0-Linux-x86_64.sh.1       getgraph.py                result_by_graph_2
bazel                                   homework                   static_analysis
bazel-0.4.5-dist.zip                    homework-data-for-student  t2t_data
bazel-0.4.5-installer-darwin-x86_64.sh  inception                  t2t_train
bazel-0.5.4-dist.zip                    inception2.zip             tensor_generator.cc
bazel-0.8.0-dist.zip                    inception_test.py          test_find_critical_path_1.py

▽
import tensorflow as tf
import numpy as np

data_dir = '/home/lyt/homework-data-for-student/'#g = np.loadtxt(data_dir + reddi)
print('load graph coo')
graph = np.loadtxt(data_dir + 'reddit.coo')
graph_coo = tf.SparseTensor(indices=graph[:, 0:2], values=graph[:, 2], dense_shape=[23297, 23297])
graph_coo = tf.cast(graph_coo, dtype=tf.float32)
def getzero(x):
    return 0

#define network
prob = tf.placeholder(shape=[], dtype=tf.float32, name='prob')
print('load features')
features = np.loadtxt(data_dir + 'reddit.feature', converters={1:getzero})
features = features[:, 2:]
features = tf.Variable(features, dtype=tf.float32, trainable=False)
y = tf.sparse_tensor_dense_matmul(graph_coo, features)
weight_fc1 = tf.get_variable(shape=[602, 400], initializer=tf.truncated_normal_initializer(stddev=0.1), name='weight_fc1')
layer1_result = tf.matmul(y, weight_fc1)
layer1_result = tf.nn.dropout(layer1_result, keep_prob=prob)

layer2_result = tf.sparse_tensor_dense_matmul(graph_coo, layer1_result)
weight_fc2 = tf.get_variable(shape=[400,200],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc2')
layer2_result = tf.matmul(layer2_result, weight_fc2)
layer2_result = tf.nn.dropout(layer2_result, keep_prob=prob)

layer3_result = tf.sparse_tensor_dense_matmul(graph_coo, layer2_result)
                                                                                                         5,23          Top
import tensorflow as tf
import numpy as np

data_dir = '/home/lyt/homework-data-for-student/'#g = np.loadtxt(data_dir + reddi)
print('load graph coo')
graph = np.loadtxt(data_dir + 'reddit.coo')
graph_coo = tf.SparseTensor(indices=graph[:, 0:2], values=graph[:, 2], dense_shape=[23297, 23297])
graph_coo = tf.cast(graph_coo, dtype=tf.float32)
def getzero(x):
    return 0

#define network
prob = tf.placeholder(shape=[], dtype=tf.float32, name='prob')
print('load features')
features = np.loadtxt(data_dir + 'reddit.feature', converters={1:getzero})
features = features[:, 2:]
features = tf.Variable(features, dtype=tf.float32, trainable=False)
y = tf.sparse_tensor_dense_matmul(graph_coo, features)
weight_fc1 = tf.get_variable(shape=[602, 400], initializer=tf.truncated_normal_initializer(stddev=0.1), name='weight_fc1')
layer1_result = tf.matmul(y, weight_fc1)
layer1_result = tf.nn.dropout(layer1_result, keep_prob=prob)

layer2_result = tf.sparse_tensor_dense_matmul(graph_coo, layer1_result)
weight_fc2 = tf.get_variable(shape=[400,200],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc2')
layer2_result = tf.matmul(layer2_result, weight_fc2)
layer2_result = tf.nn.dropout(layer2_result, keep_prob=prob)

layer3_result = tf.sparse_tensor_dense_matmul(graph_coo, layer2_result)
weight_fc3 = tf.get_variable(shape=[200,41],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc3')
                                                                                                         5,23          Top
import tensorflow as tf
import numpy as np

data_dir = '/home/lyt/homework-data-for-student/'#g = np.loadtxt(data_dir + reddi)
print('load graph coo')
graph = np.loadtxt(data_dir + 'reddit.coo')
graph_coo = tf.SparseTensor(indices=graph[:, 0:2], values=graph[:, 2], dense_shape=[23297, 23297])
graph_coo = tf.cast(graph_coo, dtype=tf.float32)
def getzero(x):
    return 0

#define network
prob = tf.placeholder(shape=[], dtype=tf.float32, name='prob')
print('load features')
features = np.loadtxt(data_dir + 'reddit.feature', converters={1:getzero})
features = features[:, 2:]
features = tf.Variable(features, dtype=tf.float32, trainable=False)
y = tf.sparse_tensor_dense_matmul(graph_coo, features)
weight_fc1 = tf.get_variable(shape=[602, 400], initializer=tf.truncated_normal_initializer(stddev=0.1), name='weight_fc1')
layer1_result = tf.matmul(y, weight_fc1)
layer1_result = tf.nn.dropout(layer1_result, keep_prob=prob)

layer2_result = tf.sparse_tensor_dense_matmul(graph_coo, layer1_result)
weight_fc2 = tf.get_variable(shape=[400,200],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc2')
layer2_result = tf.matmul(layer2_result, weight_fc2)
layer2_result = tf.nn.dropout(layer2_result, keep_prob=prob)

layer3_result = tf.sparse_tensor_dense_matmul(graph_coo, layer2_result)
weight_fc3 = tf.get_variable(shape=[200,41],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc3')
layer3_result = tf.matmul(layer3_result, weight_fc3)
#read label
print('load label')
label = np.loadtxt(data_dir + 'reddit.label', converters={1:getzero})   #(19297,43)
label = label[:,2:]
#sparse_label = tf.argmax(label, 1)
#sparse_label = tf.squeeze(sparse_label)

loss = tf.nn.softmax_cross_entropy_with_logits(logits=layer3_result[4000:, :],labels=label)
loss = tf.reduce_mean(loss)
#train_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)
train_op = tf.train.AdamOptimizer(learning_rate=0.006).minimize(loss)

test_label = np.loadtxt('/home/lyt/homework/test-label/reddit.label.full', converters={1:getzero})   #(19297,43)
test_label = test_label[:4000,2:]
test_label = tf.argmax(test_label, 1)
                                                                                                                                                                 5,23          Top
import tensorflow as tf
import numpy as np

data_dir = '/home/lyt/homework-data-for-student/'#g = np.loadtxt(data_dir + reddi)
print('load graph coo')
graph = np.loadtxt(data_dir + 'reddit.coo')
graph_coo = tf.SparseTensor(indices=graph[:, 0:2], values=graph[:, 2], dense_shape=[23297, 23297])
graph_coo = tf.cast(graph_coo, dtype=tf.float32)
def getzero(x):
    return 0

#define network
prob = tf.placeholder(shape=[], dtype=tf.float32, name='prob')
print('load features')
features = np.loadtxt(data_dir + 'reddit.feature', converters={1:getzero})
features = features[:, 2:]
features = tf.Variable(features, dtype=tf.float32, trainable=False)
y = tf.sparse_tensor_dense_matmul(graph_coo, features)
weight_fc1 = tf.get_variable(shape=[602, 400], initializer=tf.truncated_normal_initializer(stddev=0.1), name='weight_fc1')
layer1_result = tf.matmul(y, weight_fc1)
layer1_result = tf.nn.dropout(layer1_result, keep_prob=prob)

layer2_result = tf.sparse_tensor_dense_matmul(graph_coo, layer1_result)
weight_fc2 = tf.get_variable(shape=[400,200],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc2')
layer2_result = tf.matmul(layer2_result, weight_fc2)
layer2_result = tf.nn.dropout(layer2_result, keep_prob=prob)

layer3_result = tf.sparse_tensor_dense_matmul(graph_coo, layer2_result)
weight_fc3 = tf.get_variable(shape=[200,41],initializer=tf.truncated_normal_initializer(stddev=0.1), name= 'weight_fc3')
layer3_result = tf.matmul(layer3_result, weight_fc3)
#read label
print('load label')
label = np.loadtxt(data_dir + 'reddit.label', converters={1:getzero})   #(19297,43)
label = label[:,2:]
#sparse_label = tf.argmax(label, 1)
#sparse_label = tf.squeeze(sparse_label)

loss = tf.nn.softmax_cross_entropy_with_logits(logits=layer3_result[4000:, :],labels=label)
loss = tf.reduce_mean(loss)
#train_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)
train_op = tf.train.AdamOptimizer(learning_rate=0.006).minimize(loss)

test_label = np.loadtxt('/home/lyt/homework/test-label/reddit.label.full', converters={1:getzero})   #(19297,43)
test_label = test_label[:4000,2:]
test_label = tf.argmax(test_label, 1)
                                                                                                                                                                   5,23          Top